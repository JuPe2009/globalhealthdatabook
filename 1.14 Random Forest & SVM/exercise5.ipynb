{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5: Cross-validation \n",
    "\n",
    "One of the common problem for model fitting is: a model may just repeat the labels which it has seen in training dataset but it would fail to peform well when it sees a completely new data in testing data. This is called overfitting in data analysis. \n",
    "\n",
    "K-fold cross validation is one technique which is commonly used to control overfitting in clinical data anlaysis. As we demonstrated previously, we split data into a training and testing set for model fitting and evaluation. In K-fold cross validation, we split the dataset into K subsets, called K folds. Then, we iteratively fit the model K times, each time, we use the K-1 of the folds as training data and evaluating the model on the Kth fold (called validation fold). For example, if K=5, we perform 5-fold cross validation and split the data into 5 subsets. In the first iteration, we train the model with the first 4 folds and evaluate the performance on the fifth fold. In the second iteration, we train the model with the first, second, third and fifth fold and evaluate on the fourth fold. We repeat the procedure until the fifth iteration and then we average the performance metrics on each of the iteration as the final validation metrics for the fitted model. \n",
    "\n",
    "In this exercise, we will perform 5-fold cross validation with Python and scikit-learn package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform previous operations\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "data = pd.read_csv(\"./imputed_data.csv\", index_col=0)\n",
    "features = data.iloc[:, 3:-1]\n",
    "features = pd.get_dummies(features).values\n",
    "labels = data.mort_icu.values\n",
    "train_features, test_features, train_labels, test_labels = \\\n",
    "                                    train_test_split(features, labels, test_size = 0.25, random_state = 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cross validation functions \n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model we will use for each fold\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "rf_model = RandomForestClassifier(n_estimators=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the cross validation function in scikit-learn, we need to define the following hyperparameters: \n",
    "\n",
    "+ estimator: the model to fit data\n",
    "+ X: the features to feed \n",
    "+ y: the labels of data \n",
    "+ cv: the number of folds \n",
    "+ scoring: performance metrics for model\n",
    "+ n_jobs: The number of CPUs to use to do the computation\n",
    "+ return_train_score: Whether to include the training scores \n",
    "\n",
    "Same as previous exercises, we use 'accuracy' and 'roc_auc' as our performance metrics (score). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold_cv = cross_validate(rf_model, features, labels, cv=5, \n",
    "                           scoring=('accuracy', 'roc_auc'), \n",
    "                           n_jobs=8, \n",
    "                           return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy of 5-fold cross validation is 1.00 %. \n",
      "The training AUROC of 5-fold cross validation is 1.00. \n"
     ]
    }
   ],
   "source": [
    "print(\"The training accuracy of 5-fold cross validation is {:.2f} %. \".format(k_fold_cv['train_accuracy'].mean()))\n",
    "print(\"The training AUROC of 5-fold cross validation is {:.2f}. \".format(k_fold_cv['train_roc_auc'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing accuracy of 5-fold cross validation is 0.93 %. \n",
      "The testing AUROC of 5-fold cross validation is 0.88. \n"
     ]
    }
   ],
   "source": [
    "print(\"The testing accuracy of 5-fold cross validation is {:.2f} %. \".format(k_fold_cv['test_accuracy'].mean()))\n",
    "print(\"The testing AUROC of 5-fold cross validation is {:.2f}. \".format(k_fold_cv['test_roc_auc'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have performed cross validation above. Please note that: cross validation does not help you to \"solve\" overfitting problem, instead, it helps you to detect overfitting. If the performance in testing set are drastically worse than it in training set (about 20-30% worse), it means that you may consider overfitting occurs and adjust your model parameters. \n",
    "\n",
    "In the above example, we have a similar performance between training and testing dataset. The overfitting problem is not serious in this case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
