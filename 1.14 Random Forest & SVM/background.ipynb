{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background \n",
    "\n",
    "In this chapter, we are going to introduce two commonly used statisital model on healthcare data: random forest and support vector machines (SVM). These two models are mainly used in two purpose: first, to create robust and accurate predictive models. Second, These models are used in order to evaluate and interprete the features (clinical variables). The advantages of these methods: first, they both prevent overfitting and obtains reliable results; in addition, for random forest, it utilizes average ensembling to reduce bias. For SVM, the method uses kerneling to introduce non-linearity. More details will be explained in the following chapter. \n",
    "\n",
    "In the following chapter and exercises, we are going to explain and demostrate how these two models work and how to use them. \n",
    "\n",
    "## Random Forest\n",
    "\n",
    "Random forest is an ensemble model which fits multiple decision tree classifiers on subsets of the training data and uses averaging to improve the predictive score and control over-fitting[1]. To understand a random forest, we must start with the basic building block of random forest - decision tree model. \n",
    "\n",
    "### Decision Tree\n",
    "\n",
    "A decision tree model is a supervised model which learns to predict the expected output by answering a series of questions and making decisions based on the answers[2]. The concepts of decision tree model are subconsciously used by us in our daily life, for example, clinicians may intuitively evaluate a patient's condition by asking questions. We will use a clinical example to illustrate this concept further: predicting a patient's ICU mortality with the first day ICU admission data.\n",
    "\n",
    "In order to predict ICU mortality, we need to work through a series of queries. First, we may ask an initial reasonable question given the domain knowledge. We may start by asking how old the patient is. In general, the survival rate of a young patient will be higher than the elderly in ICU. After this question, we will then look at other predictive variables that could be helpful in determining a patient's ICU mortality, such as what is the gender of the patient, is there any abnormality from the laboratory results, and what kind of treatment the patient is receiving. By asking these questions, ICU clinicians would have a sense of what is the likelihood that the patient would survive in ICU. With analogy, what a decision tree model does, is similar. It designs a set of questions which separate the data with each question narrowing our possible values until we are confident enough to make a single prediction. The clinical decision tree explained is shown in Diagram 1. The complexity of a decision tree is tree detph, the number of steps from the root node to the leaf node. In Diagram 1, the depth of the decision tree is 3. In practical analysis, if the depth is too large, then your model is too complex and you will face overfitting problem. If the depth is too small, then your model will be not complex enough to capture the variance in data and you will face underfitting problem. \n",
    "\n",
    "![SVM](diagram1.png \"Decision Tree\")\n",
    "\n",
    "That is the entire high-level concept of a decision tree: a flowchart of questions leading to a prediction. Now, we take the mighty leap from a single decision tree to a random forest.\n",
    "\n",
    "### From Decision Tree to Random Forest \n",
    "\n",
    "There are many factors to take into account when the clinicians try to make a prediction. Every clinician comes to a problem with different background knowledge. If they are facing the same patient, the decisions and treatments from each clinician may differ from each other. This problem is the same for decision tree models: if looking at different sub-samples of training data, decision models may fit them with different flowcharts of questions and get different conclusions. In technical terms, there exists variance in predictions because they will widely spread around the right answer. Now if there are hundreds or thousands of clinicians, some of them are making the correct prediction, and some of them are making incorrect predictions, then all of them vote for a prediction that the majority prediction is the final decision. This is the concept of a random forest. The fundamental idea of random forest is to ensemble many decision trees into a single model to reduce the prediction variance. Individually, the prediction from human or decision trees may not be accurate, but combined, the prediction variance is reduced; thus the predictions are focused around the true outcome. \n",
    "\n",
    "In a random forest model, each decision tree only accesses a random subset of training data. This increase the diversity of the ensemble model thus improve the robustness of the overall model. This is why we call this model 'random.' When the model makes a prediction, random forest takes outputs from all individual decision tree models and output the prediction with the highest votes among individual models. In our example here, the ICU mortality prediction is a classification task. We are predicting a binary outcome of mortality (Died/Survived). The other class of problems is known as regression, where the targets are a continuous value such as ICU Free Days. In that case, the random forest will take an average for the predicted values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed, we are looking at a binary supervised classification problem (Died/Survived). For these type of problems, we may also consider another popular statistical method, support vector machine (SVM), to model the data.\n",
    "\n",
    "## Support Vector Machine (SVM) \n",
    "\n",
    "Support vector machine (SVM) is a supervised machine learning algorithm which is used for both classification and regression (output continuous predictions) purposes[3]. SVMs are more commonly employed for classification problems so we will be focusing on SVM with classification problems later. \n",
    "\n",
    "The concept of SVM is finding a plane that maximizes the margin between two classes in a dataset and achieves the best fit, as shown in Figure 1. This plane is called hyperplane. \n",
    "\n",
    "![SVM](figure1.png \"SVM1\")\n",
    "\n",
    "Support vectors are the points nearest to the hyperplane. For these points, if they are removed, the position of hyperplane would be altered to divide the dataset better. In other words, support vectors are data points (vectors) which define the hyperplane. Therefore, they are considered as the critical elements of the dataset. \n",
    "\n",
    "### What is a hyperplane? \n",
    "\n",
    "In Figure 1, for example, there are two features for the classification task. The data are in a two-dimensional space, and we can think of a hyperplane as a straight line which classifies the data into two subsets. Intuitively, the further the data points (support vectors) lie from the hyperplane, the more confident we are that they have been correctly classified. Therefore, the model will put the data points as far away as possible from the hyperplane while making sure the data points are correctly classified. So when we feed new data to the SVM model, whatever side of the hyperplane it lands will decide the class that we assign to it.\n",
    "\n",
    "### How can we identify the right hyperplane?\n",
    "\n",
    "The hyperplane is determined by the maximum margin. The margin is the distance between a hyperplane and the nearest data point from either class. The goal of fitting an SVM model is to choose a hyperplane with the highest possible margin between the hyperplane and any training data points, which means the best chance for new data to be classified correctly. We will demonstrate some scenarios on how an SVM model fit the data and identify the right hyperplane. \n",
    "\n",
    "Scenario 1 (Figure 2): Here, we have three hyperplanes (A, B and C). Now, identify the right hyperplane to classify blue dots and green dots. Apparently, “hyperplane “B” has best performed on segregation of the two classes in this scenario.\n",
    "\n",
    "![SVM scenario 1](figure2.png \"SVM1\")\n",
    "\n",
    "Scenario 2 (Figure 3): Now if all three hyperplanes (A, B, C) has segregated the classes well, then the best hyperplane will be the one that maximized the distances (margins) between nearest data point for either of the classes. In this scenario, the margin for hyperplane B is higher as compared to both A and C. Hence, we name the best hyperplane as B. An intuition behind is a hyperplane with more considerable margin is more robust, if we select a hyperplane having low margin then there is a high chance of miss-classification.\n",
    "\n",
    "![SVM scenario 2](figure3.png \"SVM2\")\n",
    "\n",
    "Scenario 3 (Figure 4): In the scenario below, we cannot have linear hyperplane between the two classes. It is where it can get tricky. Data is rarely ever as clean as our simple example above. A dataset will often look more like the mixture of balls below which represent a non-linearly separable dataset.\n",
    "\n",
    "![SVM scenario 3](figure4.png \"SVM3\")\n",
    "\n",
    "So how does SVM classify these two classes? In order to classify a dataset like this, it is necessary to move away from a 2-dimensional (2-D) view of the data to a 3-D view. Imagine that our two sets of colored balls above are sitting on a sheet and this sheet is lifted suddenly, launching the balls into the air. While the balls are up in the air, you use the sheet to separate them. This ‘lifting’ of the balls represents the mapping of data into a higher dimension. This is known as kernelling (Figure 5). \n",
    "\n",
    "![Kernelling](figure5.png \"SVM\")\n",
    "\n",
    "Kerneling is using functions to take low dimensional input space and transform it into a higher dimensional space. For example, it converts a non-separatable problem into a separable problem for classification. In Figure 4, the kernels map the 2-D data into 3-dimensional space. In the 2-D space, it is impossible to separate blue and green balls with linear functions. While after the kernel transformation maps them to 3-D space, the blue and green balls can be separated using a hyperplane. The most commonly used kernels are 'linear' kernel, 'RBF' kernel, 'polynomial' kernel and others[4]. Among them, 'RBF' kernel is the most useful in non-linear separation problems. \n",
    "\n",
    "In real life scenarios, the dimension of the data can be far higher than 3-D. For instance, we want to use a patient demographics (age, gender) and lab values to predict the ICU mortality. The number of the predictive variable is easily as high as 10+ (dimensions). In this case, the kernel we used in the SVM will require to map the 10-dimensional data into an even higher dimension in order to look for a right hyperplane. \n",
    "\n",
    "We will demonstrate how to analyze this problem with the above statistical methods with Jupyter Notebook and Python programming language in the next exercises. \n",
    "\n",
    "## Limitations of Random Forest and SVM\n",
    "\n",
    "There are also limitations in utilizing random forest in healthcare data. First, these two models are hard to interprete. For random forest, we may be able to interprete each of the decision trees. However, interpreting the ensembled random forest model is difficult for healthcare data. For SVM, due to kernel transformation, the results are also difficult to interprete. Secondly, some hyper-parameters of these methods need to be determined by users and it cannot be optimized. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "[1] Liaw, A. and Wiener, M., 2002. Classification and regression by randomForest. R news, 2(3), pp.18-22.\n",
    "\n",
    "[2] Safavian, S.R. and Landgrebe, D., 1991. A survey of decision tree classifier methodology. IEEE transactions on systems, man, and cybernetics, 21(3), pp.660-674.\n",
    "\n",
    "[3] Hearst, M.A., Dumais, S.T., Osuna, E., Platt, J. and Scholkopf, B., 1998. Support vector machines. IEEE Intelligent Systems and their applications, 13(4), pp.18-28.\n",
    "\n",
    "[4] Hsu, C.W., Chang, C.C. and Lin, C.J., 2003. A practical guide to support vector classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
